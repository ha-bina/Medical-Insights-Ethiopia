{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a5f0b43",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (387388983.py, line 6)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mfrom src.scraping.telegram_scraper import TelegramScraper``\u001b[39m\n                                                             ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "%pip install -r requirements.txt\n",
    "%pip install python-dotenv\n",
    "# src/main.py\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "from src.scraping.telegram_scraper import TelegramScraper``\n",
    "from src.scraping.channel_registry import CHANNEL_REGISTRY\n",
    "\n",
    "async def run_scraping():\n",
    "    \"\"\"Orchestrate the scraping pipeline\"\"\"\n",
    "    load_dotenv()  # Load environment variables\n",
    "    \n",
    "    scraper = TelegramScraper()\n",
    "    \n",
    "    try:\n",
    "        async with scraper.client:\n",
    "            # Scrape all registered channels\n",
    "            for channel_name in CHANNEL_REGISTRY.values():\n",
    "                success = await scraper.scrape_channel(channel_name)\n",
    "                \n",
    "                if not success:\n",
    "                    print(f\"Failed to scrape {channel_name}\")\n",
    "                else:\n",
    "                    print(f\"Successfully scraped {channel_name}\")\n",
    "                    \n",
    "    except Exception as e:\n",
    "        print(f\"Scraping failed: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(run_scraping())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4452510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of manually calling storage functions\n",
    "from datetime import datetime\n",
    "from src.scraping.storage_manager import DataLakeManager\n",
    "\n",
    "# Initialize storage\n",
    "storage = DataLakeManager()\n",
    "\n",
    "# Example data to store\n",
    "sample_data = [\n",
    "    {\n",
    "        \"id\": 123,\n",
    "        \"text\": \"Medical supplies available\",\n",
    "        \"date\": datetime.now().isoformat()\n",
    "    }\n",
    "]\n",
    "\n",
    "# Call storage function\n",
    "storage.write_to_datalake(\n",
    "    data=sample_data,\n",
    "    data_type=\"messages\",\n",
    "    channel=\"chemed\",\n",
    "    date=datetime.now().strftime('%Y-%m-%d')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87835ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# src/pipeline.py\n",
    "import asyncio\n",
    "from typing import List, Dict\n",
    "from src.scraping.telegram_scraper import TelegramScraper\n",
    "from src.scraping.storage_manager import DataLakeManager\n",
    "\n",
    "class MedicalDataPipeline:\n",
    "    def __init__(self):\n",
    "        self.scraper = TelegramScraper()\n",
    "        self.storage = DataLakeManager()\n",
    "    \n",
    "    async def execute(self):\n",
    "        \"\"\"Full pipeline execution\"\"\"\n",
    "        try:\n",
    "            async with self.scraper.client:\n",
    "                # 1. Scrape data\n",
    "                raw_data = await self._scrape_all_channels()\n",
    "                \n",
    "                # 2. Process and store\n",
    "                self._process_and_store(raw_data)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Pipeline failed: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    async def _scrape_all_channels(self) -> Dict[str, List[Dict]]:\n",
    "        \"\"\"Scrape all registered channels\"\"\"\n",
    "        from src.scraping.channel_registry import CHANNEL_REGISTRY\n",
    "        \n",
    "        results = {}\n",
    "        for channel in CHANNEL_REGISTRY.values():\n",
    "            messages = await self.scraper.scrape_channel(channel)\n",
    "            results[channel] = messages\n",
    "        return results\n",
    "    \n",
    "    def _process_and_store(self, data: Dict[str, List[Dict]]):\n",
    "        \"\"\"Process and store scraped data\"\"\"\n",
    "        for channel_name, messages in data.items():\n",
    "            # Add any preprocessing here\n",
    "            processed = self._preprocess(messages)\n",
    "            \n",
    "            # Store to data lake\n",
    "            self.storage.write_to_datalake(\n",
    "                data=processed,\n",
    "                data_type=\"messages\",\n",
    "                channel=channel_name\n",
    "            )\n",
    "    \n",
    "    def _preprocess(self, messages: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Example preprocessing\"\"\"\n",
    "        return [msg for msg in messages if msg.get('text')]  # Filter empty messages\n",
    "\n",
    "# To run the complete pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    pipeline = MedicalDataPipeline()\n",
    "    asyncio.run(pipeline.execute())\n",
    "    pip install ultralytics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0432b4e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install ultralytics\n",
    "# yolo_enrichment.py\n",
    "\n",
    "import os\n",
    "import psycopg2\n",
    "from ultralytics import YOLO\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "import uuid\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Constants\n",
    "IMAGE_DIR = Path(\"data/images\")  # directory from Task 1\n",
    "MODEL_NAME = \"yolov8n.pt\"  # use a lightweight model (or yolov8m.pt, etc.)\n",
    "\n",
    "# Connect to PostgreSQL (targeted by dbt)\n",
    "conn = psycopg2.connect(\n",
    "    host=os.getenv(\"POSTGRES_HOST\"),\n",
    "    dbname=os.getenv(\"POSTGRES_DB\"),\n",
    "    user=os.getenv(\"POSTGRES_USER\"),\n",
    "    password=os.getenv(\"POSTGRES_PASSWORD\"),\n",
    "    port=os.getenv(\"POSTGRES_PORT\")\n",
    ")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Load model\n",
    "model = YOLO(MODEL_NAME)\n",
    "\n",
    "# Scan for new images\n",
    "for image_path in IMAGE_DIR.glob(\"*.jpg\"):  # or .png\n",
    "    image_id = image_path.stem  # assume message_id is encoded in filename\n",
    "    results = model(image_path)\n",
    "\n",
    "    for result in results:\n",
    "        for box in result.boxes:\n",
    "            class_id = int(box.cls[0])\n",
    "            confidence = float(box.conf[0])\n",
    "            class_name = model.names[class_id]\n",
    "\n",
    "            # Insert detection result into staging or fact table\n",
    "            cursor.execute(\"\"\"\n",
    "                INSERT INTO staging_image_detections (detection_id, message_id, detected_object_class, confidence_score)\n",
    "                VALUES (%s, %s, %s, %s)\n",
    "            \"\"\", (\n",
    "                str(uuid.uuid4()),\n",
    "                image_id,\n",
    "                class_name,\n",
    "                confidence\n",
    "            ))\n",
    "\n",
    "conn.commit()\n",
    "cursor.close()\n",
    "conn.close()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
