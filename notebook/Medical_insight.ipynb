{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a5f0b43",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# src/main.py\n",
    "import asyncio\n",
    "from dotenv import load_dotenv\n",
    "from src.scraping.telegram_scraper import TelegramScraper\n",
    "from src.scraping.channel_registry import CHANNEL_REGISTRY\n",
    "\n",
    "async def run_scraping():\n",
    "    \"\"\"Orchestrate the scraping pipeline\"\"\"\n",
    "    load_dotenv()  # Load environment variables\n",
    "    \n",
    "    scraper = TelegramScraper()\n",
    "    \n",
    "    try:\n",
    "        async with scraper.client:\n",
    "            # Scrape all registered channels\n",
    "            for channel_name in CHANNEL_REGISTRY.values():\n",
    "                success = await scraper.scrape_channel(channel_name)\n",
    "                \n",
    "                if not success:\n",
    "                    print(f\"Failed to scrape {channel_name}\")\n",
    "                else:\n",
    "                    print(f\"Successfully scraped {channel_name}\")\n",
    "                    \n",
    "    except Exception as e:\n",
    "        print(f\"Scraping failed: {str(e)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    asyncio.run(run_scraping())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4452510",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Example of manually calling storage functions\n",
    "from datetime import datetime\n",
    "from src.scraping.storage_manager import DataLakeManager\n",
    "\n",
    "# Initialize storage\n",
    "storage = DataLakeManager()\n",
    "\n",
    "# Example data to store\n",
    "sample_data = [\n",
    "    {\n",
    "        \"id\": 123,\n",
    "        \"text\": \"Medical supplies available\",\n",
    "        \"date\": datetime.now().isoformat()\n",
    "    }\n",
    "]\n",
    "\n",
    "# Call storage function\n",
    "storage.write_to_datalake(\n",
    "    data=sample_data,\n",
    "    data_type=\"messages\",\n",
    "    channel=\"chemed\",\n",
    "    date=datetime.now().strftime('%Y-%m-%d')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87835ba0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# src/pipeline.py\n",
    "import asyncio\n",
    "from typing import List, Dict\n",
    "from src.scraping.telegram_scraper import TelegramScraper\n",
    "from src.scraping.storage_manager import DataLakeManager\n",
    "\n",
    "class MedicalDataPipeline:\n",
    "    def __init__(self):\n",
    "        self.scraper = TelegramScraper()\n",
    "        self.storage = DataLakeManager()\n",
    "    \n",
    "    async def execute(self):\n",
    "        \"\"\"Full pipeline execution\"\"\"\n",
    "        try:\n",
    "            async with self.scraper.client:\n",
    "                # 1. Scrape data\n",
    "                raw_data = await self._scrape_all_channels()\n",
    "                \n",
    "                # 2. Process and store\n",
    "                self._process_and_store(raw_data)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Pipeline failed: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    async def _scrape_all_channels(self) -> Dict[str, List[Dict]]:\n",
    "        \"\"\"Scrape all registered channels\"\"\"\n",
    "        from src.scraping.channel_registry import CHANNEL_REGISTRY\n",
    "        \n",
    "        results = {}\n",
    "        for channel in CHANNEL_REGISTRY.values():\n",
    "            messages = await self.scraper.scrape_channel(channel)\n",
    "            results[channel] = messages\n",
    "        return results\n",
    "    \n",
    "    def _process_and_store(self, data: Dict[str, List[Dict]]):\n",
    "        \"\"\"Process and store scraped data\"\"\"\n",
    "        for channel_name, messages in data.items():\n",
    "            # Add any preprocessing here\n",
    "            processed = self._preprocess(messages)\n",
    "            \n",
    "            # Store to data lake\n",
    "            self.storage.write_to_datalake(\n",
    "                data=processed,\n",
    "                data_type=\"messages\",\n",
    "                channel=channel_name\n",
    "            )\n",
    "    \n",
    "    def _preprocess(self, messages: List[Dict]) -> List[Dict]:\n",
    "        \"\"\"Example preprocessing\"\"\"\n",
    "        return [msg for msg in messages if msg.get('text')]  # Filter empty messages\n",
    "\n",
    "# To run the complete pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    pipeline = MedicalDataPipeline()\n",
    "    asyncio.run(pipeline.execute())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3d0916",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
